# ğŸ¤– Retrieval-Augmented Generation (RAG) with HuggingFace and LangChain

This project demonstrates how to build a **Retrieval-Augmented Generation (RAG)** pipeline using:

- ğŸ§  **HuggingFace QA Models** (e.g., `distilbert-base-cased-distilled-squad`)
- ğŸ” **LangChain Vector Search** (FAISS + HuggingFace embeddings)
- ğŸ“„ **Your own custom text data**
- ğŸ§± Fully modular Python design

## Features

- Vector-based document retrieval with FAISS
- Contextual question answering with HuggingFace
- RAG pattern implemented in a modular and maintainable way
- Works with CPU and GPU

## Requirements

Install dependencies:

```bash
pip install -r requirements.txt
```

## Project Structure

```
rag_pipeline/
â”œâ”€â”€ main.py
â”œâ”€â”€ load_data.py
â”œâ”€â”€ vector_store.py
â”œâ”€â”€ qa_model.py
â”œâ”€â”€ rag_runner.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## How It Works

### RAG Flow

```
[Question]
    â†“
[Retriever] â† FAISS + Embeddings
    â†“
[Context from Top-k Docs]
    â†“
[HuggingFace QA Model]
    â†“
[Answer]
```

## Running the App

```bash
python main.py
```

## Example Questions

- What is LangChain?
- What does RAG stand for?
- How does RAG work?
- What are the main features of LangChain?

## Notes

- Replace the input `text` with your own documents or load from PDFs, CSVs, etc.
- Easily extendable to Chainlit, LangGraph, or FastAPI interfaces.

## License

MIT License â€“ feel free to use, fork, and modify!